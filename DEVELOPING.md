
## Developing renku-data-services
This document details the structure of the code in this repository.
For information on how to set up a dev environment and run code, consult the [readme](/README.md).

### Coding guidelines

These can be found in our [wiki](https://github.com/SwissDataScienceCenter/renku-data-services/wiki/Coding-guidelines).

### Architecture

#### Polylith
Data Services follows a [polylith](https://polylith.gitbook.io/polylith) approach to structuring the repository
(using the [Polylith Poetry Plugin](https://davidvujic.github.io/python-polylith-docs/installation/)):
* `components` contain all the application code, divided into modules based on entity types
* `bases` contains the glue code to bring the different components together into a single unit/api. The
  entrypoint of applications is usually a `main.py` in one of the bases
* `projects` contains the Dockerfiles and pyproject.toml for each deployed service

#### Bases/Projects
There are three independent services/deployments (projects/bases):
* Renku Data Services (`data_api`): The main CRUD service for persisting data in postgres
* Secrets Storage (`secrets_storage_api`): Handles loading user secrets securely when needed
* Data Tasks (`data_tasks`): Regular background tasks
* K8s cache (`k8s_cache`): Caches Kubernetes objects so as to not overload the Kubernetes API

#### Components
Within components, there are the following modules:
* *app_config*: Application configuration file for data services
* *authn*: Authentication code (Keycloak and Gitlab)
* *authz*: Authorization code using SpiceDB/Authzed
* *base_api*: Common functionality shared by different APIs
* *base_models*: Common functionality shared by all domain models
* *base_orm*: Common functionality shared by database object relational models
* *connected_services*: Code concerning third-party integrations (e.g. Gitlab/GitHub)
* *crc*: Compute resource controls code, dealing with resource classes and resource pools for interactive compute
* *db_config*: Database configuration
* *errors*: Common application error types shared by all apis
* *k8s*: Kubernetes client code
* *message_queue*: Redis streams messaging code
* *migrations*: Database migrations
* *metrics*: Store metrics data in a staging table
* *namespace*: Code for handling namespaces (user/groups)
* *platform*: Renku platform configuration code
* *project*: Code for Project entities
* *repositories*: Code for git repositories associated with projects
* *secrets*: Code for handling user secrets
* *session*: User session management
* *storage*: Cloud storage management
* *users*: User management
* *utils*: Common utilities for reuse across the code base

This repository follows a light-weight [Hexagonal Architecture](https://en.wikipedia.org/wiki/Hexagonal_architecture_%28software%29)
approach (also known as ports and adapters).
Modules are usually split into:
* `apispec.yaml` and autogenerated from it `apispec.py`, which is the [OpenAPI](https://swagger.io/specification/) specification for endpoints.
  Customizations are done in `apispec_base.py`
* `blueprints.py` contains the [Sanic](https://sanic.dev/) endpoints for the api, dealing with (de-)serialization and validation.
* `models.py` contains domain models
* `core.py` contains business logic
* `orm.py` contains object-relational models for the database
* `db.py` contains the database repositories

The models and code in `models.py` and `core.py` form the inner circle in the architecture. This means they can be depended on
by everything, but should not depend on the outer layers. Instead, they should depend on interfaces/protocols that are
implemented in the outer layers.
The database repositories in `db.py` encapsulate database queries into a usable API. Their methods should match usecases and shouldn't
leak ORMs to the outside.
The `blueprints.py` code should call business logic in `core.py`, or if there is no business logic, call the database repositories
directly. They should always use domain models for communicating with other code.


## Supporting Services

Data Services mirrors users that it gets from [Keycloak](https://www.keycloak.org/) and Keycloak is the source of truth
for users.
We use [Authzed/SpiceDB](https://authzed.com/) for authorization. This enables complex transitive rules for who is permitted
to perform which actions.
[Redis Streams](https://redis.io/docs/latest/develop/data-types/streams/) is used for sending events to other services, mainly
[Renku Search](https://github.com/swissDataScienceCenter/renku-search) which is responsible for indexing and searching entities.


## Setting up a development environment

### Devcontainer

There is a [Devcontainer](https://containers.dev/) available in the `.devcontainer` folder.
If you use VSCode, this should be picked up automatically.
Alternatively, you can run it with the [devcontainer cli](https://github.com/devcontainers/cli) by running:
```bash
$ devcontainer up --workspace-folder .
$ devcontainer exec --container-id renku-data-services_devcontainer-data_service-1 -- bash
```
The devcontainer contains Postgres, SpiceDB, the correct Python environment and other useful development tools.

### Developing with nix

When using [nix](https://nixos.org/explore/), a development
environment can be created:

1. Run `nix develop` in the source root to drop into the development
   environment.
2. In another terminal, run `vm-run` (headless) to start a vm running
   necessary external services, like the PostgreSQL database.
3. Run `poetry install` to install the python venv

Then `make run`, `make tests` etc. can be used as usual.

The environment also contains other useful tools, like ruff-lsp,
pyright and more. Instead of a vm, a development environment using
NixOS containers is also available.

It will run a bash shell, check out [direnv](https://direnv.net/) and
the [use flake](https://direnv.net/man/direnv-stdlib.1.html#codeuse-flake-ltinstallablegtcode)
function if you prefer to keep your favorite shell.

## Running Tests

You can run style checks using `make style_checks`.
To run the test suite, use `make tests` (you likely need to run in the devcontainer for this to work, as it needs some
surrounding services to run).
* Run a specific test e.g.: `poetry run pytest test/bases/renku_data_services/data_api/test_data_connectors.py::test_create_openbis_data_connector`
* Also run tests marked with `@pytest.mark.myskip`: `PYTEST_FORCE_RUN_MYSKIPS=1 make tests`

We use [Syrupy](https://github.com/syrupy-project/syrupy) for snapshotting data in tests.

To update the snapshot data, run the following command in the devcontainer:
```bash
$ poetry run pytest -m "not schemathesis" -n auto --snapshot-update
```

### Directly from PyCharm

From the root folder of the repository, run:

1. `devcontainer build --workspace-folder .`
2. `devcontainer up --workspace-folder .`
3. `make schemas`
4. `make amalthea_schema`

> **WARNING:**
>
> Be careful with the kubernetes environment in your shell, as in case of badly setup tests and environment you might try
> to run some tests against your default cluster.

Then you can run the test as usual directly from PyCharm by clicking on the green arrow next to a specific test, or a
whole test suite or part of the test hierarchy.

We use [Syrupy](https://github.com/syrupy-project/syrupy) for snapshotting data in tests.

To update the snapshot data, run the following command in the devcontainer:
```bash
$ poetry run pytest -m "not schemathesis" -n auto --snapshot-update
```

### Directly from PyCharm

From the root folder of the repository, run:

1. `devcontainer build --workspace-folder .`
2. `devcontainer up --workspace-folder .`
3. `make schemas`
4. `make amalthea_schema`

> **WARNING:**
>
> Be careful with the kubernetes environment in your shell, as in case of badly setup tests and environment you might try
> to run some tests against your default cluster.

Then you can run the test as usual directly from PyCharm by clicking on the green arrow next to a specific test, or a
whole test suite or part of the test hierarchy.

## Migrations

We use Alembic for migrations, and we have a single version table for all schemas. This version table
is used by Alembic to determine what migrations have been applied or not and it resides in the `common`
schema. That is why all the Alembic commands include the `--name common` argument.

Our Alembic setup is such that we have multiple schemas. Most use cases will probably simply use
the `common` schema. However, if you add a new schema, you have to make sure to add the
metadata for it in the `components/renku_data_services/migrations/env.py` file.

**To create a new migration:**

`DUMMY_STORES=true poetry run alembic -c components/renku_data_services/migrations/alembic.ini --name common revision -m "<message>" --autogenerate --version-path components/renku_data_services/migrations/versions`

You can specify a different version path if you wish to, just make sure it is listed in `alembic.ini` under
`version_locations`.

**To run all migrations:**
`DUMMY_STORES=true poetry run alembic -c components/renku_data_services/migrations/alembic.ini --name=common upgrade heads`
